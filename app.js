require('dotenv').config();
const express = require('express');
const multer = require('multer');
const axios = require('axios');
const FormData = require('form-data');
const fs = require('fs');
const path = require('path');
const cors = require('cors');
const Levenshtein = require('levenshtein');
const OpenAI = require('openai');
const crypto = require('crypto');

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware
app.use(cors());
app.use(express.json());

// Configure multer for file uploads
const storage = multer.memoryStorage();
const upload = multer({
  storage: storage,
  limits: {
    fileSize: 10 * 1024 * 1024, // 10MB limit
  },
  fileFilter: (req, file, cb) => {
    if (file.fieldname === 'image') {
      if (!file.mimetype.match(/\/(jpg|jpeg|png)$/)) {
        return cb(new Error('Only JPG and PNG images are allowed'));
      }
    } else if (file.fieldname === 'audio') {
      if (!file.mimetype.match(/\/(mp3|wav|m4a|aac)$/)) {
        return cb(new Error('Only MP3, WAV, M4A, and AAC audio files are allowed'));
      }
    }
    cb(null, true);
  },
});

// ============== DEEPINFRA API (Image to Cantonese Text) ==============

/**
 * Call DeepInfra to generate Cantonese description from image
 * Uses DeepInfra's Qwen/Qwen2-VL-7B-Instruct multimodal model
 * @param {Buffer} imageBuffer - Image file buffer
 * @returns {Promise<string>} - Cantonese text description
 */
async function generateCantoneseText(imageBuffer) {
  try {
    // Initialize DeepInfra OpenAI client
    const client = new OpenAI({
      apiKey: process.env.DEEPINFRA_API_KEY,
      baseURL: 'https://api.deepinfra.com/v1/openai',
    });

    // Convert image buffer to base64
    const base64Image = imageBuffer.toString('base64');

    // Call DeepInfra's multimodal API with Qwen2.5-VL
    const response = await client.chat.completions.create({
      model: 'Qwen/Qwen2.5-VL-32B-Instruct',
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'image_url',
              image_url: {
                url: `data:image/jpeg;base64,${base64Image}`,
              },
            },
            {
              type: 'text',
              text: `ä½ æ˜¯ä¸€ä½æ“…é•¿åˆ›ä½œç²¤è¯­å­¦ä¹ å†…å®¹çš„ä½œå®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„å›¾ç‰‡å†…å®¹ï¼Œä¸ºç²¤è¯­å­¦ä¹ è€…åˆ›ä½œä¸€ä¸ªç®€çŸ­çš„åŒè¯­æ•…äº‹ã€‚

**è¦æ±‚ï¼š**
1. **æ ¼å¼**ï¼šå¿…é¡»æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

   **ï¼ˆæ™®é€šè¯ç‰ˆï¼‰**
   [ç”¨æ ‡å‡†æ™®é€šè¯è®²è¿°å›¾ç‰‡å†…å®¹ï¼Œ2-3å¥è¯]

   **ï¼ˆç²¤è¯­ç‰ˆï¼‰**
   [ç”¨åœ°é“ç²¤è¯­å£è¯­è®²è¿°åŒæ ·çš„å†…å®¹ï¼Œ2-3å¥è¯]

2. **é•¿åº¦**ï¼šæ¯ä¸ªç‰ˆæœ¬æ§åˆ¶åœ¨**2-3å¥è¯**ï¼Œç®€æ´ç²¾ç‚¼ï¼Œä¾¿äºè·Ÿè¯»ã€‚

3. **å†…å®¹**ï¼š
   - **åŸºäºå›¾ç‰‡**ï¼šç´§å¯†å›´ç»•å›¾ç‰‡ä¸­çš„æ ¸å¿ƒå…ƒç´ ï¼ˆäººç‰©ã€ç‰©ä½“ã€åœºæ™¯ï¼‰ã€‚
   - **ä¿æŒä¸€è‡´**ï¼šä¸¤ä¸ªç‰ˆæœ¬è®²è¿°çš„æ˜¯åŒä¸€ä¸ªåœºæ™¯ï¼Œåªæ˜¯è¯­è¨€ä¸åŒã€‚
   - **èå…¥æ–‡åŒ–**ï¼šå¯è‡ªç„¶èå…¥å¹¿åºœåœ°åŒºæ—¥å¸¸ç”Ÿæ´»å…ƒç´ ï¼ˆå¦‚é¥®èŒ¶ã€è¡ŒèŠ±è¡—ã€è½é›¨æ”¶è¡«ç­‰ï¼‰ã€‚
   - **ç§¯ææœ‰è¶£**ï¼šæ•´ä½“åŸºè°ƒè½»æ¾ã€æ¸©é¦¨ã€‚

4. **è¯­è¨€é£æ ¼**ï¼š
   - æ™®é€šè¯ç‰ˆï¼šä½¿ç”¨æ ‡å‡†æ™®é€šè¯ä¹¦é¢è¯­
   - ç²¤è¯­ç‰ˆï¼šä½¿ç”¨åœ°é“ç²¤è¯­å£è¯­ï¼ˆå¦‚ï¼šå‘¢åº¦ã€å—°åº¦ã€å’ã€å””ã€ä½¢ç­‰ï¼‰

5. **è¾“å‡ºæ ¼å¼**ï¼šåªè¾“å‡ºä¸Šè¿°ä¸¤éƒ¨åˆ†å†…å®¹ï¼Œæ— éœ€å…¶ä»–è§£é‡Šã€‚

**ç¤ºä¾‹å‚è€ƒï¼ˆå¦‚æœå›¾ç‰‡æ˜¯ä¸€æ¯å¥¶èŒ¶å’Œä¸€æœ¬ä¹¦ï¼‰ï¼š**
**ï¼ˆæ™®é€šè¯ç‰ˆï¼‰**
ä»Šå¤©ä¸‹åˆï¼Œå°æ˜å·å·å»äº†æ¥¼ä¸‹æ–°å¼€çš„èŒ¶é¤å…ï¼Œç‚¹äº†ä¸€æ¯å†»å¥¶èŒ¶ã€‚ä»–æ‹¿ç€ä¹¦è£…æ–‡è‰ºï¼Œç»“æœçœ‹ç€çœ‹ç€ï¼Œå¤ªä¸“æ³¨å–å¥¶èŒ¶ï¼Œä¸å°å¿ƒæ»´äº†ä¸¤æ»´åœ¨ä¹¦ä¸Šã€‚

**ï¼ˆç²¤è¯­ç‰ˆï¼‰**
ä»Šæ—¥ä¸‹æ˜¼ï¼Œé˜¿æ˜å·å·èµ°å’—å»æ¥¼ä¸‹æ–°å¼€å˜…èŒ¶è®°ï¼Œå«å’—æ¯å†»å¥¶èŒ¶ã€‚ä½¢æ‹ä½æœ¬ä¹¦æ‰®æ–‡é’ï¼Œç‚¹çŸ¥ç‡ç‡ä¸‹ä¹¦ï¼ŒæŒ‚ä½é¥®å¥¶èŒ¶ï¼Œæ»´å’—ä¸¤æ»´è½æœ¬ä¹¦åº¦ã€‚

**ç°åœ¨ï¼Œè¯·æ ¹æ®æˆ‘æä¾›çš„å›¾ç‰‡å†…å®¹å¼€å§‹åˆ›ä½œï¼š**`,
            },
          ],
        },
      ],
      max_tokens: 500,
      temperature: 0.8,
    });

    // Extract the generated Cantonese text
    const cantoneseText = response.choices[0]?.message?.content?.trim();

    if (!cantoneseText) {
      throw new Error('Empty response from DeepInfra API');
    }

    return cantoneseText;

  } catch (error) {
    console.error('DeepInfra API Error:', error.message);
    if (error.response) {
      console.error('DeepInfra API Response:', error.response.data);
    }
    throw new Error(`Failed to generate Cantonese text: ${error.message}`);
  }
}

/**
 * Fallback: Generate Chinese description then translate to Cantonese story
 * This is used when the direct Cantonese story generation fails
 * @param {Buffer} imageBuffer - Image file buffer
 * @returns {Promise<string>} - Cantonese story text
 */
async function generateCantoneseStoryWithFallback(imageBuffer) {
  try {
    console.log('Attempting fallback: Chinese â†’ Cantonese story translation...');

    // Initialize DeepInfra OpenAI client
    const client = new OpenAI({
      apiKey: process.env.DEEPINFRA_API_KEY,
      baseURL: 'https://api.deepinfra.com/v1/openai',
    });

    // Convert image buffer to base64
    const base64Image = imageBuffer.toString('base64');

    // Step 1: Generate Chinese description first
    const chineseDescriptionPrompt = `è¯·ç”¨ç®€ä½“ä¸­æ–‡æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„åœºæ™¯ï¼ŒåŒ…æ‹¬äººç‰©ã€åŠ¨ä½œå’ŒèƒŒæ™¯ã€‚æ§åˆ¶åœ¨2-3å¥è¯ï¼Œç®€æ´æ˜äº†ã€‚`;

    const chineseResponse = await client.chat.completions.create({
      model: 'Qwen/Qwen2.5-VL-32B-Instruct',
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'image_url',
              image_url: {
                url: `data:image/jpeg;base64,${base64Image}`,
              },
            },
            {
              type: 'text',
              text: chineseDescriptionPrompt,
            },
          ],
        },
      ],
      max_tokens: 300,
      temperature: 0.7,
    });

    const chineseText = chineseResponse.choices[0]?.message?.content?.trim();

    if (!chineseText) {
      throw new Error('Failed to generate Chinese description in fallback');
    }

    console.log('Chinese description generated:', chineseText.substring(0, 50) + '...');

    // Step 2: Translate and adapt to bilingual story
    const translationPrompt = `ä½ æ˜¯ä¸€ä½æ“…é•¿åˆ›ä½œç²¤è¯­å­¦ä¹ å†…å®¹çš„ä½œå®¶ã€‚æˆ‘å°†æä¾›ä¸€æ®µç®€ä½“ä¸­æ–‡çš„å›¾ç‰‡æè¿°ï¼Œè¯·ä½ å°†å…¶æ”¹ç¼–æˆåŒè¯­å­¦ä¹ å†…å®¹ã€‚

**è¦æ±‚ï¼š**
1. **æ ¼å¼**ï¼šå¿…é¡»æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

   **ï¼ˆæ™®é€šè¯ç‰ˆï¼‰**
   [ä½¿ç”¨æä¾›çš„ä¸­æ–‡æè¿°ï¼Œç¨ä½œæ¶¦è‰²ï¼Œ2-3å¥è¯]

   **ï¼ˆç²¤è¯­ç‰ˆï¼‰**
   [ç”¨åœ°é“ç²¤è¯­å£è¯­ç¿»è¯‘ä¸Šè¿°å†…å®¹ï¼Œ2-3å¥è¯]

2. **é•¿åº¦**ï¼šæ¯ä¸ªç‰ˆæœ¬æ§åˆ¶åœ¨**2-3å¥è¯**ï¼Œç®€æ´ç²¾ç‚¼ã€‚
3. **ç²¤è¯­é£æ ¼**ï¼šä½¿ç”¨åœ°é“ç²¤è¯­å£è¯­ï¼ˆå¦‚ï¼šå‘¢åº¦ã€å—°åº¦ã€å’ã€å””ã€ä½¢ç­‰ï¼‰
4. **è¾“å‡ºæ ¼å¼**ï¼šåªè¾“å‡ºä¸Šè¿°ä¸¤éƒ¨åˆ†å†…å®¹ï¼Œæ— éœ€å…¶ä»–è§£é‡Šã€‚

**ä¸­æ–‡æè¿°ï¼š**
${chineseText}

**è¯·æ”¹ç¼–æˆåŒè¯­å†…å®¹ï¼š**`;

    const cantoneseResponse = await client.chat.completions.create({
      model: 'Qwen/Qwen2.5-VL-32B-Instruct',
      messages: [
        {
          role: 'user',
          content: translationPrompt,
        },
      ],
      max_tokens: 500,
      temperature: 0.8,
    });

    const cantoneseStory = cantoneseResponse.choices[0]?.message?.content?.trim();

    if (!cantoneseStory) {
      throw new Error('Failed to translate to Cantonese story in fallback');
    }

    console.log('Fallback succeeded: Generated Cantonese story via translation');

    return cantoneseStory;

  } catch (error) {
    console.error('Fallback process failed:', error.message);
    throw new Error(`Fallback generation failed: ${error.message}`);
  }
}

// ============== STEPFUN TTS API (Text to Cantonese Speech) ==============

/**
 * Intelligent voice selection based on story content analysis
 * Analyzes the story type, characters, and mood to recommend appropriate voice
 * @param {string} text - Story text to analyze
 * @returns {string} - Recommended voice ID
 *
 * Available voices from step-tts-mini:
 * Male: cixingnansheng, zhengpaiqingnian, yuanqinansheng, qingniandaxuesheng,
 *       boyinnansheng, ruyananshi, shenchennanyin
 * Female: qinqienvsheng, wenrounvsheng, jilingshaonv, yuanqishaonv,
 *         ruanmengnvsheng, youyanvsheng, lengyanyujie, shuangkuaijiejie,
 *         wenjingxuejie, linjiajiejie, linjiameimei, zhixingjiejie
 */
function selectIntelligentVoice(text) {
  // Analysis keywords for story categorization
  const analysis = {
    // Refined children's story detection - removed generic "å¼€å¿ƒ" and standalone particles
    // Only match specific child-related terms and particle combinations
    isChildrenStory: /å°æœ‹å‹|ç»†è·¯|ç»†è·¯ä»”|å°å­©|å°å­©ä»”|å„¿ç«¥|å…’ç«¥|ç©è€|ç©æœ¨å—|å˜»å˜»å“ˆå“ˆ|ç«¥çœŸ|æ­ç§¯æœ¨|æ­é«˜å¡”|ç»†è·¯ä»”å‘€|ç»†è·¯ä»”å’§|å°å­©å‘€|å°å­©å’¯/.test(text),

    // Adult/adolence indicators - if these are present, it's NOT a children's story
    // Expanded with more professions, vehicles, and adult-related terms
    hasAdultIndicators: /å¸‚æ°‘|é”»ç‚¼|å¥èº«|è¿åŠ¨|å·¥ä½œ|ä¸Šç­|å…¬å¸|èŒå‘˜|æˆäºº|æˆå¹´äºº|é’å¹´|è€äºº|å¸æœº|è€æ¿|èŒå‘˜|åº—å‘˜|é¡¾å®¢|è¡—åŠ|é‚»å±…|è­¦å¯Ÿ|åŒ»ç”Ÿ|æŠ¤å£«|è€å¸ˆ|å­¦ç”Ÿ|å¼€è½¦|é©¾è½¦|éª‘è½¦|è·¯äºº|è¡Œäºº|ä¹˜å®¢|åº—ä¸»|å•†è´©|å‘˜å·¥/.test(text),

    // Warm emotional stories
    isWarmEmotional: /æ¸©æš–|æ¸©é¦¨|å¹¸ç¦|æ‹¥æŠ±|äº²äºº|å®¶äºº|å©†å©†|å…¬å…¬|å®¶äººå›¢èš|äº²æƒ…|æ„ŸåŠ¨|æ¸©é¦¨/.test(text),

    // Educational content
    isEducational: /å­¦ä¹ |å­¸ç¿’|è¯»ä¹¦|ç‡æ›¸|è¯¾å ‚|èª²å ‚|å­¦æ ¡|å­¸æ ¡|çŸ¥è¯†|çŸ¥è­˜|æ•™å­¦|æ•™æ|å­¦ä¹ ç­|è¡¥ä¹ /.test(text),

    // Daily life scenes (Hong Kong style) - expanded with street, shop, vehicle scenes
    isDailyLife: /æ—¥å¸¸ç”Ÿæ´»|ç”Ÿæ´»|è¡—å¸‚|èŒ¶é¤å…|èŒ¶è®°|é¥®èŒ¶|åƒé¥­|é£Ÿé£¯|è½é›¨|æ”¶è¡«|ä¸Šç­|æ”¾å·¥|ä¹°èœ|ç…®é¥­|å¨æˆ¿|å¨|åšé¥­|çƒ¹é¥ª|ç…®é£Ÿ|å®¶é‡Œ|å±‹ä¼|å®¶|å®¶åº­|å®¶å±…|é¤å…|é¥­å…|è¡—é“|è¡—è¾¹|è¡—å¸‚|é©¬è·¯|è·¯|åº—é“º|é“ºå¤´|å•†åº—|å•†åœº|è´­ç‰©|é€›è¡—|å¼€è½¦|é©¾è½¦|éª‘è½¦|åè½¦|ä¹˜è½¦|äº¤é€š|å¡è½¦|å µè½¦|è½¦æµ/.test(text),

    // Fitness/Sports scenes (NEW)
    isFitnessSports: /å¥èº«|é”»ç‚¼|è¿åŠ¨|è·‘æ­¥|æ‰“çƒ|æ¸¸æ³³|ç‘œä¼½|åšè¿åŠ¨|ä½“èƒ½|è®­ç»ƒ|æ“åœº|ä½“è‚²é¦†|å™¨æ/.test(text),

    // Calm narrative - expanded with more tranquility keywords
    isCalmNarrative: /é™é™|éœéœ|æ…¢æ…¢|æ¼¸æ¼¸|è½»è½»|è¼•è¼•|ç¼“ç¼“|ç·©ç·©|å¹³é™|å¹³éœ|å®‰éœ|æ‚ é—²|æ‚ é–’|æ”¾æ¾|æ”¾é¬†|å®é™|å¯§éœ|å’Œè°|å’Œè«§|èˆ’é€‚|èˆ’é©|æƒ¬æ„|äº«å—|å®‰è¯¦|å®‰è©³/.test(text),

    // Energetic/active content - expanded keywords
    isEnergetic: /å˜»å˜»å“ˆå“ˆ|å“ˆå“ˆ|å˜»å˜»|çƒ­çƒˆ|ç†±çƒˆ|çƒ­é—¹|ç†±é¬§|æ¬¢å¿«|æ­¡å¿«|è·³è·ƒ|è·³èº|è·‘|å†²|è¡|æ´»åŠ›|è¿åŠ¨|å¥èº«|é”»ç‚¼|å……æ»¡æ´»åŠ›|ç²¾ç¥|å…ƒæ°”|å¿™ç¢Œ|ç´§å¼ |æ¿€çƒˆ/.test(text),

    // Protagonist gender detection - expanded with profession titles
    hasMaleProtagonist: /å°æ˜|é˜¿æ˜|å“¥å“¥|é˜¿å“¥|çˆ¸çˆ¸|å…¬å…¬|å…ˆç”Ÿ|ç”·äºº|ç”·å­|ç”·ç”Ÿ|å°ä¼™å­|ç”·å­©|ç”·ä»”|å¸ˆå‚…|å¨å¸ˆ|çˆ¶äº²|çˆ·çˆ·|å”å”|ä¼¯ä¼¯/.test(text),
    hasFemaleProtagonist: /å°ç¾|é˜¿ç¾|å§å§|å®¶å§|å¦¹å¦¹|ç´°å¦¹|å¦ˆå¦ˆ|å©†å©†|å¥³äºº|å¥³å­|å¥³ç”Ÿ|å¥³ä»”|å§‘å¨˜|å¥³å­©|é˜¿å©†|æ¯äº²|å¥¶å¥¶|é˜¿å§¨|å¸ˆå§|å¸ˆå¦¹/.test(text),
  };

  // Override: If adult indicators are present, force isChildrenStory to false
  if (analysis.hasAdultIndicators) {
    analysis.isChildrenStory = false;
  }

  console.log('Story content analysis:', analysis);

  // Decision logic for voice selection
  let selectedVoice;

  // Priority 1: Children's stories (most specific)
  if (analysis.isChildrenStory) {
    if (analysis.hasFemaleProtagonist || text.includes('å¦¹å¦¹') || text.includes('å®¶å§')) {
      selectedVoice = 'linjiameimei'; // Young girl voice for children's stories
      console.log('Selected: linjiameimei (children\'s story with female characters)');
    } else if (analysis.hasMaleProtagonist || text.includes('å“¥å“¥') || text.includes('é˜¿å“¥')) {
      selectedVoice = 'yuanqinansheng'; // Young boy voice
      console.log('Selected: yuanqinansheng (children\'s story with male characters)');
    } else {
      selectedVoice = 'yuanqishaonv'; // Default youthful female for general children's content
      console.log('Selected: yuanqishaonv (default children\'s story voice)');
    }
  }
  // Priority 2: Fitness/Sports scenes (NEW)
  else if (analysis.isFitnessSports) {
    if (analysis.hasFemaleProtagonist) {
      selectedVoice = 'jilingshaonv'; // Energetic female for fitness
      console.log('Selected: jilingshaonv (fitness story with female characters)');
    } else {
      selectedVoice = 'zhengpaiqingnian'; // Energetic young male for fitness
      console.log('Selected: zhengpaiqingnian (fitness story with male/neutral characters)');
    }
  }
  // Priority 3: Warm emotional stories
  else if (analysis.isWarmEmotional) {
    if (analysis.hasFemaleProtagonist) {
      selectedVoice = 'qinqienvsheng'; // Intimate and gentle female
      console.log('Selected: qinqienvsheng (warm emotional story with female protagonist)');
    } else {
      selectedVoice = 'yuanqinansheng'; // Warm male voice
      console.log('Selected: yuanqinansheng (warm emotional story with male protagonist)');
    }
  }
  // Priority 4: Educational content
  else if (analysis.isEducational) {
    if (analysis.hasFemaleProtagonist) {
      selectedVoice = 'wenjingxuejie'; // Scholarly female student
      console.log('Selected: wenjingxuejie (educational content with female voice)');
    } else {
      selectedVoice = 'boyinnansheng'; // Clear broadcast male
      console.log('Selected: boyinnansheng (educational content with male voice)');
    }
  }
  // Priority 5: Daily life stories
  else if (analysis.isDailyLife) {
    selectedVoice = 'shuangkuaijiejie'; // Cheerful sisterly for daily life
    console.log('Selected: shuangkuaijiejie (daily life story)');
  }
  // Priority 6: Energetic/active content
  else if (analysis.isEnergetic) {
    selectedVoice = 'jilingshaonv'; // Smart and lively
    console.log('Selected: jilingshaonv (energetic story)');
  }
  // Priority 7: Calm narrative
  else if (analysis.isCalmNarrative) {
    selectedVoice = 'wenrounvsheng'; // Soft and warm
    console.log('Selected: wenrounvsheng (calm narrative)');
  }
  // Default: Protagonist-based selection
  else {
    if (analysis.hasFemaleProtagonist) {
      selectedVoice = 'wenrounvsheng'; // Gentle female default
      console.log('Selected: wenrounvsheng (default female protagonist)');
    } else if (analysis.hasMaleProtagonist) {
      selectedVoice = 'cixingnansheng'; // Magnetic male default
      console.log('Selected: cixingnansheng (default male protagonist)');
    } else {
      selectedVoice = 'wenrounvsheng'; // Overall default
      console.log('Selected: wenrounvsheng (overall default)');
    }
  }

  return selectedVoice;
}

/**
 * Call StepFun API to synthesize Cantonese speech from text
 * Uses StepFun's step-tts-2 model with intelligent voice selection
 * @param {string} text - Cantonese text to synthesize
 * @param {string} [voiceOverride] - Optional voice ID to override intelligent selection
 * @returns {Promise<Buffer>} - Audio buffer (MP3 format)
 */
async function synthesizeCantoneseSpeech(text, voiceOverride) {
  try {
    // Use intelligent voice selection if no override provided
    const voiceId = voiceOverride || selectIntelligentVoice(text);

    console.log(`Synthesizing speech with voice: ${voiceId}`);

    const response = await axios.post(
      `${process.env.STEPFUN_API_ENDPOINT || 'https://api.stepfun.com/v1'}/audio/speech`,
      {
        model: process.env.STEPFUN_MODEL || 'step-tts-2',
        input: text,
        voice: voiceId,
        response_format: 'mp3',
        speed: 1.0,
      },
      {
        headers: {
          'Authorization': `Bearer ${process.env.STEPFUN_API_KEY}`,
          'Content-Type': 'application/json',
        },
        responseType: 'arraybuffer',
        timeout: 60000,
      }
    );

    console.log(`Speech synthesis successful with voice: ${voiceId}`);
    // Return audio buffer
    return Buffer.from(response.data);

  } catch (error) {
    console.error('StepFun API Error:', error.message);
    if (error.response) {
      console.error('StepFun API Response:', error.response.data.toString());
    }
    throw new Error(`Failed to synthesize speech: ${error.message}`);
  }
}

// ============== DEEPINFRA WHISPER API (Cantonese Speech Recognition) ==============

/**
 * Call DeepInfra Whisper to recognize Cantonese speech
 * Uses OpenAI's Whisper model via DeepInfra REST API
 * @param {Buffer} audioBuffer - Audio file buffer
 * @returns {Promise<{text: string, confidence: number}>} - Recognized text and confidence score
 */
async function recognizeCantoneseSpeech(audioBuffer) {
  try {
    const model = process.env.WHISPER_MODEL || 'openai/whisper-large-v3';

    // Create form data with audio file
    const FormData = require('form-data');
    const form = new FormData();

    // Append audio buffer as a file
    form.append('audio', audioBuffer, {
      filename: 'audio.mp3',
      contentType: 'audio/mp3',
    });

    // Append parameters
    form.append('model', model);
    form.append('language', 'zh'); // Chinese (will handle both Mandarin and Cantonese)
    form.append('response_format', 'verbose_json'); // Get detailed response with timestamps

    // Call DeepInfra's Whisper REST API
    const response = await axios.post(
      `https://api.deepinfra.com/v1/openai/whisper`,
      form,
      {
        headers: {
          ...form.getHeaders(),
          'Authorization': `Bearer ${process.env.DEEPINFRA_API_KEY}`,
        },
        timeout: 60000,
      }
    );

    // Extract the recognized text
    const text = response.data.text?.trim() || '';

    if (!text) {
      throw new Error('Empty response from Whisper API');
    }

    // Calculate average confidence from segments if available
    let confidence = 0.95; // Default high confidence
    if (response.data.segments && response.data.segments.length > 0) {
      // Use average probability from segments as confidence
      const avgProbability = response.data.segments.reduce(
        (sum, seg) => sum + (seg.avg_logprob || 0),
        0
      ) / response.data.segments.length;
      // Convert logprob to confidence (rough approximation)
      confidence = Math.max(0.5, Math.min(1.0, (avgProbability + 2) / 4));
    }

    console.log(`Whisper recognized text: ${text.substring(0, 50)}...`);

    return {
      text: text,
      confidence: confidence,
    };

  } catch (error) {
    console.error('DeepInfra Whisper API Error:', error.message);
    if (error.response) {
      console.error('DeepInfra Whisper Response:', error.response.data);
    }
    throw new Error(`Failed to recognize speech: ${error.message}`);
  }
}

// ============== SCORING LOGIC ==============

/**
 * Calculate pronunciation score based on text similarity and confidence
 * @param {string} originalText - Original Cantonese text
 * @param {string} userText - User's recognized speech text
 * @param {number} confidence - ASR confidence score (0-1)
 * @returns {Object} - Score breakdown
 */
function calculatePronunciationScore(originalText, userText, confidence) {
  // Normalize texts for comparison
  const normalize = (text) => text.toLowerCase().replace(/[^\u4e00-\u9fa5a-z0-9]/g, '');

  const normalizedOriginal = normalize(originalText);
  const normalizedUser = normalize(userText);

  // Calculate Levenshtein distance
  const distance = new Levenshtein(normalizedOriginal, normalizedUser).distance;
  const maxLen = Math.max(normalizedOriginal.length, normalizedUser.length) || 1;
  const similarity = 1 - (distance / maxLen);

  // Calculate comprehensive score
  // 70% weight on text similarity, 30% on ASR confidence
  const score = Math.round((similarity * 0.7 + confidence * 0.3) * 100);

  // Determine accuracy level
  let accuracy = 'Poor';
  if (score >= 90) accuracy = 'Excellent';
  else if (score >= 75) accuracy = 'Good';
  else if (score >= 60) accuracy = 'Fair';

  // Estimate fluency based on score
  const fluency = Math.min(100, Math.round(score * 0.9 + 10));

  return {
    score: Math.max(0, Math.min(100, score)),
    accuracy,
    fluency,
    similarity: Math.round(similarity * 100),
  };
}

// ============== API ENDPOINTS ==============

/**
 * Health check endpoint
 */
app.get('/health', (req, res) => {
  res.json({ success: true, message: 'Server is running', timestamp: new Date().toISOString() });
});

/**
 * POST /api/generate
 * Generate Cantonese text and speech from uploaded image
 */
app.post('/api/generate', upload.single('image'), async (req, res) => {
  try {
    // Validate request
    if (!req.file) {
      return res.status(400).json({
        success: false,
        error: 'No image file provided. Please upload an image with field name "image"',
      });
    }

    console.log('Processing image:', req.file.originalname);

    // Step 1: Generate Cantonese story from image
    let cantoneseText;
    try {
      cantoneseText = await generateCantoneseText(req.file.buffer);
      console.log('Generated story:', cantoneseText);
    } catch (primaryError) {
      console.warn('Primary generation failed, attempting fallback...', primaryError.message);
      try {
        cantoneseText = await generateCantoneseStoryWithFallback(req.file.buffer);
        console.log('Fallback generation succeeded');
      } catch (fallbackError) {
        console.error('Both primary and fallback generation failed:', fallbackError.message);
        throw new Error(`Failed to generate Cantonese story: ${primaryError.message}. Fallback also failed: ${fallbackError.message}`);
      }
    }

    // Step 2: Synthesize speech from Cantonese text
    const audioBuffer = await synthesizeCantoneseSpeech(cantoneseText);
    console.log('Synthesized audio size:', audioBuffer.length, 'bytes');

    // Step 3: Convert audio to base64 for client
    const audioBase64 = audioBuffer.toString('base64');
    const audioUrl = `data:audio/mp3;base64,${audioBase64}`;

    // Return success response
    res.json({
      success: true,
      data: {
        text: cantoneseText,
        audioUrl: audioUrl,
        audioFormat: 'mp3',
        type: 'story', // New identifier for story format
      },
    });

  } catch (error) {
    console.error('Generate endpoint error:', error);
    res.status(500).json({
      success: false,
      error: error.message || 'Failed to generate Cantonese content',
    });
  }
});

/**
 * POST /api/evaluate
 * Evaluate user's Cantonese pronunciation
 */
app.post('/api/evaluate', upload.single('audio'), async (req, res) => {
  try {
    // Validate request
    if (!req.file) {
      return res.status(400).json({
        success: false,
        error: 'No audio file provided. Please upload an audio file with field name "audio"',
      });
    }

    const originalText = req.body.originalText;
    if (!originalText) {
      return res.status(400).json({
        success: false,
        error: 'Missing originalText in request body',
      });
    }

    console.log('Evaluating audio:', req.file.originalname);
    console.log('Original text:', originalText);

    // Step 1: Recognize user's speech
    const { text: userText, confidence } = await recognizeCantoneseSpeech(req.file.buffer);
    console.log('Recognized text:', userText, 'Confidence:', confidence);

    // Step 2: Calculate pronunciation score
    const scoreData = calculatePronunciationScore(originalText, userText, confidence);

    // Return success response
    res.json({
      success: true,
      data: {
        originalText: originalText,
        userText: userText,
        score: scoreData.score,
        accuracy: scoreData.accuracy,
        fluency: scoreData.fluency,
        similarity: scoreData.similarity,
        confidence: Math.round(confidence * 100),
      },
    });

  } catch (error) {
    console.error('Evaluate endpoint error:', error);
    res.status(500).json({
      success: false,
      error: error.message || 'Failed to evaluate pronunciation',
    });
  }
});

/**
 * Error handling middleware
 */
app.use((error, req, res, next) => {
  if (error instanceof multer.MulterError) {
    if (error.code === 'LIMIT_FILE_SIZE') {
      return res.status(400).json({
        success: false,
        error: 'File size exceeds the 10MB limit',
      });
    }
  }

  if (error.message.includes('Only') || error.message.includes('allowed')) {
    return res.status(400).json({
      success: false,
      error: error.message,
    });
  }

  console.error('Unhandled error:', error);
  res.status(500).json({
    success: false,
    error: 'Internal server error',
  });
});

// ============== START SERVER ==============
app.listen(PORT, () => {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ¤ æ‹ç…§å­¦ç²¤è¯­ API Server Started ğŸ¤             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Server running on: http://localhost:${PORT}                  â•‘
â•‘  Health check:    GET  /health                             â•‘
â•‘  Generate:        POST /api/generate                       â•‘
â•‘  Evaluate:        POST /api/evaluate                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… APIs configured:
   - DeepInfra Vision (Qwen2.5-VL-32B-Instruct)
   - StepFun TTS (step-tts-2)
   - DeepInfra Whisper (whisper-large-v3)

âš ï¸  Make sure all required environment variables are set!
`);
});

module.exports = app;
